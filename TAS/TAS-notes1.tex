\documentclass[12pt]{article}

%opening
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}
\begin{itemize}
\item Let the random variables Y1 and Y2 be distributed bivariate normal with E(Y1) = $\mu_1$, E(Y2) = $\mu_2$, var(Y1) = $\sigma^2_1$ and $\sigma^2_2$.
\item Correlation coefficient $-1 < \rho  < 1$. 
\item Of particular interest are tests of the unconditional
marginal hypotheses (equal means and equal variances) and tests of the joint
hypothesis (simultaneous test)
\item \textbf{Casewise sums and differences:} The random variables $D = Y1-Y2$ and $S = Y1 + Y2$ are bivariate normal with expectations E(D) = $mu_D$ = $\mu_1$ - $\mu_2$, and E(S) = $mu_S$ = $\mu_1$ + $\mu_2$;
\item We show that the test procedure for $H_J$ advanced by Bradley and Blackwood
(1989) \textbf{additively} decomposes into independent tests of $H_3$ and the conditional marginal hypothesis $H_2 : \mu_1 = \mu_2$; assuming the additional restriction of equal variance.

%-------------------------------------------------------------------%

\item \textbf{Section 2.1 Pitman-Morgan Test}

\end{itemize}


%-------------------------------------------------------------------%
%Section 3
\section{Regression of Differences on Sums}

\begin{itemize}
\item Casewise difference for pairing $i$ : $D_i$. Corresponding casewise sum : $s_i$. 
\item Regression Equation : $D_i = \beta_0 + \beta_1 s_i + \varepsilon_i$ for $i=1,2,\ldots n$.
\item ($\varepsilon_i$ is i.i.d  and normally distributed)
\item \textbf{Mean Centering} - Reformulation
\[ D_i = \alpha + \beta(s_i -\bar{s}) + \varepsilon_i\]
Here
\[ \alpha = \mu_D + beta_1(\bar{s} - \mu_s)\]
\end{itemize}

\subsection{MLE estimates of Regression Parameters}

\begin{eqnarray}
\hat{\alpha} &=& \bar{D} \\
\hat{\beta}_1 &=& \frac{\sum(D_i - \bar{D}}{s_i - \bar{s}}
\end{eqnarray}
%---------------------------%
\newpage
\subsection{F-tests}

Two independent F- ratio Tests

\begin{itemize}
\item Test of no intercept parameter
\[ F^{\ast}_0 = \frac{n\bar{D}^2}{\sigma^2_\varepsilon} \sim F_{df = (1,n-2)}\]

\item Test of no slope parameter
\[ F^{\ast}_1 = \frac{ \bat{\beta}^2_1\sum(s_i-\bar{s}^2}{\sigma^2_\varepsilon} \sim F_{df = (1,n-2)}\]
\end{itemize}

\[ F^{\ast}  = \frac{F^{\ast}_0 + F^{\ast}_1 }{2} \sim F_{df = (2,n-2)}\]
\begin{itemize}
\item Alternative Representation of Blackwood-Bradley's Test Statisitc
\end{itemize}
%---------------------------%
\textit{F-test notes}
\begin{itemize}
\item Most F-tests arise by considering a decomposition of the variability in a collection of data in terms of sums of squares. 
\end{itemize}
%-------------------------------------------------------------------%
%% SECTION 4
% Page 6
\section{Alternative Derivation}
\textbf{Notation}
\begin{itemize}
\item
\item $ E(S) = \mu_s.\boldsybol{1}.$
\item Distribution of D conditional on $\tilde{S}$ (D|S).
\[boldsymbol{S} - \bar{S}I = JS\]
\item $\boldsymbol{J} = (I - n^{-1}11^{T}$ (J is idempotent). The matrix $\boldsymbol{J}$ is not of full rank.

\end{itemize}


%-------------------------------------------------------------------%
%Page 7 Top
\[ \frac{ \rho^2_{DS} }{ n- (n-1)\rho^2_{DS}  }  \mbox{ for } i \neq j\]


%-------------------------------------------------------------------%
%Page 7 Center


\end{document}
